{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Story data analyses\n",
    "---\n",
    "\n",
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "import numpy as np\n",
    "import glob as gl\n",
    "import plotly.plotly as pl\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "from scipy import stats as st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import data\n",
    "import csv files that were exported from 'dataframe_processing_and_export.ipynb' and then moved to EXPORT folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch story data from csv files stored in /documents and with filename format \"initials_type.csv\" (e.g. og_pm, og_su). Store the fetched data as pandas dataframes inside dictionary d\n",
    "storydata1 = gl.glob('EXPORT/*_pm.csv')\n",
    "storydata2 = gl.glob('EXPORT/*_su.csv')\n",
    "\n",
    "d1 = {}#pm DBs\n",
    "\n",
    "d_raw = {}#su DBs\n",
    "\n",
    "#load csvs as dataframes into separate dictionaries\n",
    "for story in storydata1:\n",
    "    d1[story[7:12]] = pd.read_csv(story, sep=',', encoding='latin-1')\n",
    "\n",
    "for story in storydata2:\n",
    "    d_raw[story[7:12]] = pd.read_csv(story, sep=',', encoding='latin-1')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions\n",
    "\n",
    "### pre-processing\n",
    "\n",
    "- **format_timeVals**: prepares all imported time data for any further processing.\n",
    "\n",
    "- **distribute_ids_to_adjacent_places**: takes raw story units as inputs and allocates new IDs to these units based on whether they are adjacent to story units referring to the same place.\n",
    "\n",
    "- **consolidate_adjacent_places**: takes the data processed by distribute_ids_to_adjacent_places and aggregates rows based on the newly-allocated IDs into the dictionary of dataframes entitled *d_con*.\n",
    "\n",
    "- **add_su_buffer**: adds temporal buffers of user's choice (in seconds) to the input dataframe.\n",
    "\n",
    "### statistics\n",
    "\n",
    "- **new_timeVals**: calculate new fields time_length, which provides each story unit length as a timedelta value, and num_minutes, which provides this value as a real number.\n",
    "\n",
    "- **calcStats**: calculates some statistics based on story unit / place mention dynamics. That is, whether a place mention with a *time* val falls between *time_start* and *time_end* vals (>=,<). When a mention occurs at the temporal breakpoint between two story units, the story unit that follows the first is the one that the mention becomes associated with [...]\n",
    "\n",
    "### aggregation\n",
    "\n",
    "- **aggregatedByPlaces**: input data is copied into a new dictionary entitled *d_byPlace* and aggregated by place name, producing data whereby each row represents a totality of attributes pertaining to the story units that related to the said place.\n",
    "\n",
    "- **timeSum**: is called from within aggregatedByPlaces to enable the sum of the time_length field during aggregation.\n",
    "\n",
    "---\n",
    "the following cell needs to be run before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timeVals(name, df):\n",
    "    if name[-2:] == 'pm':\n",
    "        df['time'] = pd.to_datetime(df.time, format='%Y-%m-%d %H:%M:%S')\n",
    "    else:\n",
    "        df['time_start'] = pd.to_datetime(df.time_start, format='%Y-%m-%d %H:%M:%S')\n",
    "        df['time_end'] = pd.to_datetime(df.time_end, format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#attribute new IDs to story units so that each unique ID represents a change in spatial discourse instead of a story unit change (i.e. each ID represents a spatially distinct chunk of discourse)\n",
    "def distribute_ids_to_adjacent_places(name, df):\n",
    "    for i in df.index:\n",
    "        if df.loc[i, 'id'] == 1:\n",
    "            df.loc[i, 'agg_su_id'] = df.loc[i, 'id']\n",
    "        elif df.loc[i, 'place'] == df.loc[i-1, 'place']:\n",
    "            df.loc[i, 'agg_su_id'] = df.loc[i-1, 'agg_su_id']\n",
    "        else:\n",
    "            df.loc[i, 'agg_su_id'] = df.loc[i, 'id']\n",
    "\n",
    "#aggregate these rows by these new IDs\n",
    "def consolidate_adjacent_places(name, df, append):\n",
    "    d_raw[name + append] = df.groupby(['agg_su_id','place','scale_order'], as_index=False).agg({\n",
    "        'id': lambda x: x.astype('str').str.cat(sep=';'),\n",
    "        'su_num': lambda x: x.astype('str').str.cat(sep=';'),\n",
    "        'time_start':'first',\n",
    "        'time_end':'last'})\n",
    "    d_raw[name + append] = d_raw[name + append][['agg_su_id','id','su_num','place','scale_order','time_start','time_end']]\n",
    "        \n",
    "def add_su_buffer(name, df, buffer):\n",
    "    #subtract timedelta object from each datetime object in column and assign new values\n",
    "    df['time_start'] = df['time_start'].map(lambda x: x - td(seconds=buffer))\n",
    "    df['time_end'] = df['time_end'].map(lambda x: x + td(seconds=buffer))\n",
    "    #add 5 seconds to first value to make sure it remains at 00:00:00\n",
    "    df.iloc[0]['time_start'] = df.iloc[0]['time_start'] + td(seconds=buffer)\n",
    "\n",
    "def new_timeVals(name, df):\n",
    "    df['time_length'] = df.time_end - df.time_start\n",
    "    df['num_minutes'] = df['time_length'].map(lambda x: x.total_seconds()/60)\n",
    "    #exported pd.datetime values are imported and need to be trimmed into datetime objects w format HH:MM:SS\n",
    "    #df['time_length'] = pd.to_datetime(df.time_length.str.replace('.000000000','').str.replace('[0-9]+ days?,? ',''), format='%H:%M:%S')\n",
    "    #this datetime object is then converted into a list of 3 time values representing [hours,minutes,seconds]\n",
    "    df['time_length'] = df['time_length'].map(lambda x: str(x).split(' ')[2].split(':'))        \n",
    "    \n",
    "def calcStats(name1, df1, name2, df2):\n",
    "    \n",
    "    r_pm = range(len(df1.index))\n",
    "    r_su = range(len(df2.index))\n",
    "    \n",
    "    df2['mention_freq'] = 0\n",
    "    df2['mention_index'] = ''\n",
    "    df2['mention_places'] = ''\n",
    "    \n",
    "    df2['mention_match_freq'] = 0\n",
    "    df2['mention_match_index'] = ''\n",
    "    df2['mention_match_places'] = ''\n",
    "    \n",
    "    df2['mention_coarser_match_freq'] = 0\n",
    "    df2['mention_coarser_match_index'] = ''\n",
    "    df2['mention_coarser_match_places'] = ''\n",
    "    \n",
    "    df2['mention_finer_match_freq'] = 0\n",
    "    df2['mention_finer_match_index'] = ''\n",
    "    df2['mention_finer_match_places'] = ''\n",
    "    \n",
    "    try:\n",
    "         for i in r_su:\n",
    "            a = dates.date2num(df2.iloc[i]['time_start'])\n",
    "            b = dates.date2num(df2.iloc[i]['time_end'])\n",
    "            place1 = df2.iloc[i]['place']\n",
    "            x = 0\n",
    "            x1 = ''\n",
    "            x11 = ''\n",
    "            y = 0\n",
    "            y1 = ''\n",
    "            y11 = ''\n",
    "            y2 = 0\n",
    "            y21 = ''\n",
    "            y22 = ''\n",
    "            y3 = 0\n",
    "            y31 = ''\n",
    "            y32 = ''\n",
    "\n",
    "            for j in r_pm:\n",
    "                c = dates.date2num(df1.iloc[j]['time'])\n",
    "                if (c >= a) and (c < b):\n",
    "                    _id = str(df1.iloc[j]['id'])\n",
    "                    place2 = df1.iloc[j]['place']\n",
    "\n",
    "                    x += 1\n",
    "                    x1 += (\";\" + _id)\n",
    "                    x11 += (\";\" + place2)\n",
    "                    if place2 == place1:\n",
    "                        y += 1\n",
    "                        y1 += (\";\" + _id)\n",
    "                        y11 += (\";\" + place2)\n",
    "                    elif place1 in place2:\n",
    "                        y2 += 1\n",
    "                        y21 += (\";\" + _id)\n",
    "                        y22 += (\";\" + place2)\n",
    "                    elif place2 in place1:\n",
    "                        y3 += 1\n",
    "                        y31 += (\";\" + _id)\n",
    "                        y32 += (\";\" + place2)\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                df2.iloc[i, df2.columns.get_loc('mention_freq')] = x\n",
    "                df2.iloc[i, df2.columns.get_loc('mention_index')] = x1\n",
    "                df2.iloc[i, df2.columns.get_loc('mention_places')] = x11\n",
    "\n",
    "                df2.iloc[i, df2.columns.get_loc('mention_match_freq')] = y\n",
    "                df2.iloc[i, df2.columns.get_loc('mention_match_index')] = y1\n",
    "                df2.iloc[i, df2.columns.get_loc('mention_match_places')] = y11\n",
    "                \n",
    "                df2.iloc[i, df2.columns.get_loc('mention_finer_match_freq')] = y2\n",
    "                df2.iloc[i, df2.columns.get_loc('mention_finer_match_index')] = y21\n",
    "                df2.iloc[i, df2.columns.get_loc('mention_finer_match_places')] = y22\n",
    "                \n",
    "                df2.iloc[i, df2.columns.get_loc('mention_coarser_match_freq')] = y3\n",
    "                df2.iloc[i, df2.columns.get_loc('mention_coarser_match_index')] = y31\n",
    "                df2.iloc[i, df2.columns.get_loc('mention_coarser_match_places')] = y32\n",
    "\n",
    "\n",
    "    except:\n",
    "        print(name1, name2, place1, place2, i, j, _id)\n",
    "        raise\n",
    "\n",
    "#look at unit-mention dynamics by place (total of all spatial units in a same place within a story), instead of spatial units.\n",
    "def aggregatedByPlaces(name, df, append):\n",
    "    d_raw[name+append] = df.groupby(['place','scale_order'], as_index=False).agg({\n",
    "        'agg_su_id': lambda x: x.astype('str').str.replace('.0','').str.cat(sep=';'), #duplicates need to be removed\n",
    "        'id': lambda x: x.astype('str').str.cat(sep=';'),\n",
    "        'su_num': lambda x: x.astype('str').str.cat(sep=';'),\n",
    "        'time_start':'first',\n",
    "        'time_end':'last',\n",
    "        'time_length': lambda x: timeSum(x),\n",
    "        'num_minutes': 'sum',\n",
    "        'mention_index': lambda x: x.astype('str').str.cat(sep='').strip(';').replace('nan',''),\n",
    "        'mention_match_index': lambda x: x.astype('str').str.cat(sep='').strip(';').replace('nan',''),\n",
    "        'mention_coarser_match_index': lambda x: x.astype('str').str.cat(sep='').strip(';').replace('nan',''),\n",
    "        'mention_finer_match_index': lambda x: x.astype('str').str.cat(sep='').strip(';').replace('nan',''),\n",
    "        'mention_places': lambda x: x.astype('str').str.cat(sep='').strip(';').replace('nan',''),\n",
    "        'mention_coarser_match_places': lambda x: x.astype('str').str.cat(sep='').strip(';').replace('nan',''),\n",
    "        'mention_match_places': lambda x: x.astype('str').str.cat(sep='').strip(';').replace('nan',''),\n",
    "        'mention_finer_match_places': lambda x: x.astype('str').str.cat(sep='').strip(';').replace('nan',''),\n",
    "        'mention_freq':'sum',\n",
    "        'mention_coarser_match_freq':'sum',\n",
    "        'mention_match_freq':'sum',\n",
    "        'mention_finer_match_freq':'sum'})\n",
    "    \n",
    "def timeSum(x):\n",
    "    #input for this function is a list of strings representing time in the format HH:MM:SS\n",
    "    tdeltas = []\n",
    "    for i in x:\n",
    "        #x is a list whose values reflect the grouping logic of the groupby function.\n",
    "        #timedeltas are represented in absolute seconds\n",
    "        tdeltas.append(td(hours=int(i[0]),minutes=int(i[1]),seconds=int(i[2])))\n",
    "    #adding a timedelta object (td) to the sum will provoke the sums output to be in td format\n",
    "    return sum(tdeltas, td())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executing the functions\n",
    "---\n",
    "running these functions uses loops to cycle through each story within a dictionary. Each dictionary contains a copy of all 10 stories to which these functions need to be applied in a different order depending on what we want to test.\n",
    "\n",
    "For the present analysis, we will apply 3 database formats to all 10 stories, each will require its own pipeline of functions, the formats include:\n",
    "1. a format with each aggregated story unit and attributes about that unit's relation with simultaneous mentions of the place.\n",
    "2. a format with each aggregated story unit (with its temporal boundaries extended by 5 seconds) and attributes about that unit's relation with simultaneous mentions of the place.*\n",
    "3. a format with each aggregated story unit (with its temporal boundaries extended by 10 seconds) and attributes about that unit's relation with simultaneous mentions of the place.*\n",
    "\n",
    "*adding buffers at the beginning and end, but especially beginning, of spatal discourse units will make up for a story unit that may have started after a mention which provoked it.\n",
    "\n",
    "each of the previous databases can be aggregated by place so as to produce more holistic and summative views on the relation between each individual place in a story and simultaneous mentions of that place.\n",
    "\n",
    "4. a format with each unique place in a story and attributes about that place's relation with simultaneous mentions of the place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's format the place mentions first, once and for all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in d1.items():\n",
    "    format_timeVals(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create 3 new dictionaries from d_raw to contain these 3 formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in zip(list(d_raw.keys()),list(d_raw.values())):\n",
    "    format_timeVals(k, v)\n",
    "    distribute_ids_to_adjacent_places(k, v)\n",
    "    consolidate_adjacent_places(k, v, '_2')\n",
    "    consolidate_adjacent_places(k, v, '_2_b5')\n",
    "    consolidate_adjacent_places(k, v, '_2_b10')\n",
    "#if we are to reuse the same consolidate_adjacent_places function, we need to pass an argument that will generate a new df within the same dictionary (we cannot dynamically create new dictionaries using an argument passed to a new function, but we CAN create new dfs within an existing dict this way!)\n",
    "\n",
    "#from this, create new dictionaries... for simplicity's sake\n",
    "\n",
    "d_2 = {}#consolidated su DBs\n",
    "d_2_b5 = {}#consolidated and expanded su DBs (5 seconds before beginning of each story unit and 5 seconds after)\n",
    "d_2_b10 = {}#consolidated and expanded su DBs (10 seconds before beginning of each story unit and 10 seconds after)\n",
    "\n",
    "#distribute these new dataframes to the new dicts\n",
    "for k, v in d_raw.items():\n",
    "    if k[-1:] == '2':\n",
    "        d_2[k] = pd.DataFrame(v)\n",
    "    elif k[-1:] == '5':\n",
    "        d_2_b5[k] = pd.DataFrame(v)\n",
    "    elif k[-1:] == '0':\n",
    "        d_2_b10[k] = pd.DataFrame(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. a format with each aggregated story unit and attributes about that unit's relation with simultaneous place mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (k1, v1), (k2, v2) in zip(sorted(d1.items()), sorted(d_2.items())):\n",
    "    new_timeVals(k2, v2)\n",
    "    calcStats(k1, v1, k2, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. a format with each aggregated story unit (with its temporal boundaries extended by 5 seconds) and attributes about that unit's relation with simultaneous mentions of the place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/emory/Desktop/geomedialab_lifestories_timeseries/venv/lib/python3.5/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (k1, v1), (k2, v2) in zip(sorted(d1.items()), sorted(d_2_b5.items())):\n",
    "    add_su_buffer(k2, v2, 5)\n",
    "    new_timeVals(k2, v2)\n",
    "    calcStats(k1, v1, k2, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. a format with each aggregated story unit (with its temporal boundaries extended by 10 seconds) and attributes about that unit's relation with simultaneous mentions of the place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/emory/Desktop/geomedialab_lifestories_timeseries/venv/lib/python3.5/site-packages/ipykernel_launcher.py:32: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (k1, v1), (k2, v2) in zip(sorted(d1.items()), sorted(d_2_b10.items())):\n",
    "    add_su_buffer(k2, v2, 10)\n",
    "    new_timeVals(k2, v2)\n",
    "    calcStats(k1, v1, k2, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to also produce new dataframes that will contain\n",
    "\n",
    "1. each story broken down by unique place, rather than changes in spatial discourse. Therefore, we will be able to view a summary of each distinct place's relation to story units. We also want to see \n",
    "2. an aggregation of all these places across 10 stories, so we can have a more totalizing view on how scale plays into this relationship. \n",
    "\n",
    "We want these datasets for all three dictionaries (i.e. (1) the one with 5-second extended spatial discourse units, (2) the one with 10-second extended spatial discourse units, (3) the one with unchanged spatial discourse units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in d_2.items():\n",
    "    aggregatedByPlaces(k, v, '_p')\n",
    "for k, v in d_2_b5.items():\n",
    "    aggregatedByPlaces(k, v, '_p')\n",
    "for k, v in d_2_b10.items():\n",
    "    aggregatedByPlaces(k, v, '_p')\n",
    "\n",
    "d_2_p = {}#consolidated su -> place-aggregated DBs\n",
    "d_2_b5_p = {}#consolidated and expanded su DBs (5 seconds before beginning of each story unit and 5 seconds after) -> place-aggregated DBs\n",
    "d_2_b10_p = {}#consolidated and expanded su DBs (10 seconds before beginning of each story unit and 10 seconds after) ->place-aggregated DBs\n",
    "\n",
    "#the previous function added these new dataframes to our d_raw dictionary. Let's copy them to their own dictionaries for simplicity\n",
    "for k, v in d_raw.items():\n",
    "    if k[-3:] == '2_p':\n",
    "        d_2_p[k] = pd.DataFrame(v)\n",
    "    elif k[-3:] == '5_p':\n",
    "        d_2_b5_p[k] = pd.DataFrame(v)\n",
    "    elif k[-3:] == '0_p':\n",
    "        d_2_b10_p[k] = pd.DataFrame(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now aggregate dataframes by new story unit ids (generated by 'consolidate_adjacent_places') using 'aggregatedByAggId' and then aggregated by place name using 'aggregatedByPlaces'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['eh_su_2', 'ek_su_2', 'fv_su_2', 'ct_su_2', 'ep_su_2', 'jr_su_2', 'bn_su_2', 'ap_su_2', 'og_su_2', 'jm_su_2'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_2.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_2 = pd.concat(d_2)\n",
    "total_b5 = pd.concat(d_2_b5)\n",
    "total_b10 = pd.concat(d_2_b10)\n",
    "\n",
    "p_all = aggregatedByPlaces('2', total_2, '_p_all')\n",
    "b5_p_all = aggregatedByPlaces('2_b5', total_b5, '_p_all')\n",
    "b10_p_all = aggregatedByPlaces('2_b10', total_b10, '_p_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agg_su_id                              float64\n",
       "id                                      object\n",
       "su_num                                  object\n",
       "place                                   object\n",
       "scale_order                              int64\n",
       "time_start                      datetime64[ns]\n",
       "time_end                        datetime64[ns]\n",
       "time_length                             object\n",
       "num_minutes                            float64\n",
       "mention_freq                             int64\n",
       "mention_index                           object\n",
       "mention_places                          object\n",
       "mention_match_freq                       int64\n",
       "mention_match_index                     object\n",
       "mention_match_places                    object\n",
       "mention_coarser_match_freq               int64\n",
       "mention_coarser_match_index             object\n",
       "mention_coarser_match_places            object\n",
       "mention_finer_match_freq                 int64\n",
       "mention_finer_match_index               object\n",
       "mention_finer_match_places              object\n",
       "dtype: object"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_2['eh_su_2'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate tables showing regression analyses btwn amount of minutes of discourse time regarding a given place (x) and number of simultaneous mentions of that place (y).\n",
    "\n",
    "*the output here needs to be played with manually. Change the values inside the regression_table function to control which data are being tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intercept</th>\n",
       "      <th>n</th>\n",
       "      <th>p_value</th>\n",
       "      <th>r_value</th>\n",
       "      <th>slope</th>\n",
       "      <th>std_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ap_su_2</th>\n",
       "      <td>2.843333</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.097278</td>\n",
       "      <td>-0.733200</td>\n",
       "      <td>-1.131667</td>\n",
       "      <td>0.524788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bn_su_2</th>\n",
       "      <td>1.216667</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.004745</td>\n",
       "      <td>0.943216</td>\n",
       "      <td>2.341667</td>\n",
       "      <td>0.412342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ct_su_2</th>\n",
       "      <td>0.999921</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.625110</td>\n",
       "      <td>1.452969</td>\n",
       "      <td>0.349149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eh_su_2</th>\n",
       "      <td>2.089431</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.144358</td>\n",
       "      <td>0.300538</td>\n",
       "      <td>0.932927</td>\n",
       "      <td>0.617346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ek_su_2</th>\n",
       "      <td>1.675926</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.944681</td>\n",
       "      <td>-0.021399</td>\n",
       "      <td>-0.080093</td>\n",
       "      <td>1.128240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ep_su_2</th>\n",
       "      <td>1.869350</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.305459</td>\n",
       "      <td>0.186996</td>\n",
       "      <td>0.538079</td>\n",
       "      <td>0.516089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fv_su_2</th>\n",
       "      <td>1.134702</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.930381</td>\n",
       "      <td>0.014662</td>\n",
       "      <td>0.014702</td>\n",
       "      <td>0.167113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jm_su_2</th>\n",
       "      <td>1.266866</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.804444</td>\n",
       "      <td>0.042133</td>\n",
       "      <td>0.093214</td>\n",
       "      <td>0.373624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jr_su_2</th>\n",
       "      <td>4.475000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.594278</td>\n",
       "      <td>0.595033</td>\n",
       "      <td>0.908333</td>\n",
       "      <td>1.226869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>og_su_2</th>\n",
       "      <td>3.778283</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.435952</td>\n",
       "      <td>0.226606</td>\n",
       "      <td>0.773737</td>\n",
       "      <td>0.960030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.134948</td>\n",
       "      <td>20.3</td>\n",
       "      <td>0.426186</td>\n",
       "      <td>0.217969</td>\n",
       "      <td>0.584387</td>\n",
       "      <td>0.627559</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         intercept     n   p_value   r_value     slope   std_err\n",
       "ap_su_2   2.843333   6.0  0.097278 -0.733200 -1.131667  0.524788\n",
       "bn_su_2   1.216667   6.0  0.004745  0.943216  2.341667  0.412342\n",
       "ct_su_2   0.999921  29.0  0.000288  0.625110  1.452969  0.349149\n",
       "eh_su_2   2.089431  25.0  0.144358  0.300538  0.932927  0.617346\n",
       "ek_su_2   1.675926  13.0  0.944681 -0.021399 -0.080093  1.128240\n",
       "ep_su_2   1.869350  32.0  0.305459  0.186996  0.538079  0.516089\n",
       "fv_su_2   1.134702  38.0  0.930381  0.014662  0.014702  0.167113\n",
       "jm_su_2   1.266866  37.0  0.804444  0.042133  0.093214  0.373624\n",
       "jr_su_2   4.475000   3.0  0.594278  0.595033  0.908333  1.226869\n",
       "og_su_2   3.778283  14.0  0.435952  0.226606  0.773737  0.960030\n",
       "mean      2.134948  20.3  0.426186  0.217969  0.584387  0.627559"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def regression_table(name, df):\n",
    "    keys = ['slope','intercept','r_value','p_value','std_err','n']\n",
    "    newtable[name] = dict.fromkeys(keys)    \n",
    "    #slope, intercept, r_value, p_value, std_err = st.linregress(df.loc[df['scale_order'] == 3, 'mention_match_freq'],df.loc[df['scale_order'] == 3, 'num_minutes'])\n",
    "    #newtable[name] = {'slope':slope,'intercept':intercept,'r_value':r_value,'p_value':p_value,'std_err':std_err,'n':df.loc[df['scale_order'] == 3].shape[0]}\n",
    "    slope, intercept, r_value, p_value, std_err = st.linregress(df.loc[(df['scale_order'] == 3) | (df['scale_order'] == 2), 'mention_coarser_match_freq'],df.loc[(df['scale_order'] == 3) | (df['scale_order'] == 2), 'num_minutes'])\n",
    "    newtable[name] = {'slope':slope,'intercept':intercept,'r_value':r_value,'p_value':p_value,'std_err':std_err,'n':df.loc[(df['scale_order'] == 3) | (df['scale_order'] == 2)].shape[0]}\n",
    "    \n",
    "def append_mean(df):\n",
    "    mean_row = df.mean()\n",
    "    mean_df = pd.DataFrame(mean_row)\n",
    "    mean_df.columns = ['mean']\n",
    "    mean_df = mean_df.transpose()\n",
    "    df = df.append(mean_df)\n",
    "    return df\n",
    "\n",
    "newtable = {}\n",
    "for k, v in d_2.items():\n",
    "    regression_table(k, v)\n",
    "stats_df = pd.DataFrame(newtable).transpose()\n",
    "stats_df = append_mean(stats_df)\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 500\n",
    "#display(d22['ep_su'])\n",
    "#d2['ap_su'].loc[d2['ap_su']['scale_order'] == 6].shape[0]\n",
    "#print(d22['bn_su'].shape[0], total['bn']\n",
    "\n",
    "#aggregatedByPlaces('total',total_agg)\n",
    "def searchit(d_raw, search_id):\n",
    "    for k, v in d_raw.items():\n",
    "        print(k)\n",
    "        print(v.loc[v['id'] == search_id].place)\n",
    "\n",
    "searchit(d_raw, 13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intercept</th>\n",
       "      <th>n</th>\n",
       "      <th>p_value</th>\n",
       "      <th>r_value</th>\n",
       "      <th>slope</th>\n",
       "      <th>std_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2_all_p</th>\n",
       "      <td>2.332115</td>\n",
       "      <td>88.0</td>\n",
       "      <td>6.494124e-07</td>\n",
       "      <td>0.501391</td>\n",
       "      <td>2.411058</td>\n",
       "      <td>0.448651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_b10_all_p</th>\n",
       "      <td>3.128464</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.323567e-05</td>\n",
       "      <td>0.446114</td>\n",
       "      <td>1.827318</td>\n",
       "      <td>0.395303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_b5_all_p</th>\n",
       "      <td>2.896135</td>\n",
       "      <td>88.0</td>\n",
       "      <td>1.077113e-05</td>\n",
       "      <td>0.450214</td>\n",
       "      <td>1.884786</td>\n",
       "      <td>0.403094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.785571</td>\n",
       "      <td>88.0</td>\n",
       "      <td>8.218737e-06</td>\n",
       "      <td>0.465907</td>\n",
       "      <td>2.041054</td>\n",
       "      <td>0.415683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             intercept     n       p_value   r_value     slope   std_err\n",
       "2_all_p       2.332115  88.0  6.494124e-07  0.501391  2.411058  0.448651\n",
       "2_b10_all_p   3.128464  88.0  1.323567e-05  0.446114  1.827318  0.395303\n",
       "2_b5_all_p    2.896135  88.0  1.077113e-05  0.450214  1.884786  0.403094\n",
       "mean          2.785571  88.0  8.218737e-06  0.465907  2.041054  0.415683"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression_table('2_b10_all_p', d_raw['2_b10_p_all'])\n",
    "stats_df = pd.DataFrame(newtable).transpose()\n",
    "append_mean(stats_df.iloc[0:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intercept</th>\n",
       "      <th>n</th>\n",
       "      <th>p_value</th>\n",
       "      <th>r_value</th>\n",
       "      <th>slope</th>\n",
       "      <th>std_err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2_all</th>\n",
       "      <td>2.567988</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>7.051967e-10</td>\n",
       "      <td>0.894761</td>\n",
       "      <td>1.009669</td>\n",
       "      <td>0.102857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_all_p</th>\n",
       "      <td>2.567988</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>7.051967e-10</td>\n",
       "      <td>0.894761</td>\n",
       "      <td>1.009669</td>\n",
       "      <td>0.102857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_b10_all</th>\n",
       "      <td>3.285109</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>5.011017e-10</td>\n",
       "      <td>0.897855</td>\n",
       "      <td>1.038059</td>\n",
       "      <td>0.103908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_b5_all</th>\n",
       "      <td>2.460418</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>3.256865e-10</td>\n",
       "      <td>0.901622</td>\n",
       "      <td>1.047032</td>\n",
       "      <td>0.102528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ap_su_2_p</th>\n",
       "      <td>-0.726059</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1.100208e-06</td>\n",
       "      <td>0.996812</td>\n",
       "      <td>0.577452</td>\n",
       "      <td>0.020670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bn_su_2_p</th>\n",
       "      <td>-6.558942</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.540827e-02</td>\n",
       "      <td>0.944873</td>\n",
       "      <td>2.513371</td>\n",
       "      <td>0.502863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ct_su_2_p</th>\n",
       "      <td>-0.594874</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.756650e-05</td>\n",
       "      <td>0.998254</td>\n",
       "      <td>1.348526</td>\n",
       "      <td>0.046070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eh_su_2_p</th>\n",
       "      <td>-1.858104</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.056778e-01</td>\n",
       "      <td>0.986254</td>\n",
       "      <td>1.178642</td>\n",
       "      <td>0.197470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ek_su_2_p</th>\n",
       "      <td>-11.849295</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.227607e-02</td>\n",
       "      <td>0.977724</td>\n",
       "      <td>3.980363</td>\n",
       "      <td>0.604218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ep_su_2_p</th>\n",
       "      <td>0.229423</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.746016e-03</td>\n",
       "      <td>0.982609</td>\n",
       "      <td>0.769250</td>\n",
       "      <td>0.083929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fv_su_2_p</th>\n",
       "      <td>-0.657435</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.623295e-08</td>\n",
       "      <td>0.987777</td>\n",
       "      <td>0.572910</td>\n",
       "      <td>0.031964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jm_su_2_p</th>\n",
       "      <td>-0.242669</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.263447e-03</td>\n",
       "      <td>0.997737</td>\n",
       "      <td>1.248901</td>\n",
       "      <td>0.059518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jr_su_2_p</th>\n",
       "      <td>0.204689</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.436426e-04</td>\n",
       "      <td>0.990198</td>\n",
       "      <td>1.765574</td>\n",
       "      <td>0.124519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>og_su_2_p</th>\n",
       "      <td>-2.861102</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2.143880e-05</td>\n",
       "      <td>0.952266</td>\n",
       "      <td>2.548852</td>\n",
       "      <td>0.288886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-1.002347</td>\n",
       "      <td>11.642857</td>\n",
       "      <td>1.061611e-02</td>\n",
       "      <td>0.957393</td>\n",
       "      <td>1.472019</td>\n",
       "      <td>0.169447</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           intercept          n       p_value   r_value     slope   std_err\n",
       "2_all       2.567988  26.000000  7.051967e-10  0.894761  1.009669  0.102857\n",
       "2_all_p     2.567988  26.000000  7.051967e-10  0.894761  1.009669  0.102857\n",
       "2_b10_all   3.285109  26.000000  5.011017e-10  0.897855  1.038059  0.103908\n",
       "2_b5_all    2.460418  26.000000  3.256865e-10  0.901622  1.047032  0.102528\n",
       "ap_su_2_p  -0.726059   7.000000  1.100208e-06  0.996812  0.577452  0.020670\n",
       "bn_su_2_p  -6.558942   5.000000  1.540827e-02  0.944873  2.513371  0.502863\n",
       "ct_su_2_p  -0.594874   5.000000  8.756650e-05  0.998254  1.348526  0.046070\n",
       "eh_su_2_p  -1.858104   3.000000  1.056778e-01  0.986254  1.178642  0.197470\n",
       "ek_su_2_p -11.849295   4.000000  2.227607e-02  0.977724  3.980363  0.604218\n",
       "ep_su_2_p   0.229423   5.000000  2.746016e-03  0.982609  0.769250  0.083929\n",
       "fv_su_2_p  -0.657435  10.000000  9.623295e-08  0.987777  0.572910  0.031964\n",
       "jm_su_2_p  -0.242669   4.000000  2.263447e-03  0.997737  1.248901  0.059518\n",
       "jr_su_2_p   0.204689   6.000000  1.436426e-04  0.990198  1.765574  0.124519\n",
       "og_su_2_p  -2.861102  10.000000  2.143880e-05  0.952266  2.548852  0.288886\n",
       "mean       -1.002347  11.642857  1.061611e-02  0.957393  1.472019  0.169447"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
