{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to start, we import the needed python libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dates\n",
    "import glob as gl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then import the spreadsheet (.csv) data into python 'pandas' dataframes.\n",
    "\n",
    "Several modifications to the original data were carried out on the data being imported:\n",
    "- Some 'journey' story units (FV 15-18, EP 32-35, OG 63-65) were combined into one since they had been erroneously created as several story units consisting of the same journey. cumulative time stamps were erased to prevent journey segments from rendering on graphs. Refer to clip_time or to original db for reference.\n",
    "- OG story units 39 and 40 to correct time error.\n",
    "- CT: 7 journeys were in original dataset, but only one was retained in atlascine. This is because these 6 other journeys recount movement of others (not the storyteller/protagonist). These journeys were kept in the updated datasets and therefore render on the present graphs.\n",
    "\n",
    "Because this data is public, and certain fields revealed the identity of the storyteller, the following columns have been removed for the current datasets\n",
    "- '*_pm.csv' data sets have columns H-L removed (index 7-10)\n",
    "- '*_su.csv' data sets have columns J, K, and N-W removed (index 9, 10, 13-22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch story data from csv files stored in /documents and with filename format \"initials_type.csv\" (e.g. og_pm, og_su). Store the fetched data as pandas dataframes inside dictionary d\n",
    "storydata1 = gl.glob('documents_place_mentions/*.csv')\n",
    "storydata2 = gl.glob('documents_story_units/*.csv')\n",
    "i = 0\n",
    "j = 0\n",
    "d1 = {}\n",
    "d2 = {}\n",
    "\n",
    "#load csvs as dataframes into a dictionary\n",
    "for story in storydata1:\n",
    "    d1[story[25:30]] = pd.read_csv(story, sep=',', encoding='latin-1')\n",
    "    i += 1\n",
    "\n",
    "for story in storydata2:\n",
    "    d2[story[22:27]] = pd.read_csv(story, sep=',', encoding='latin-1')\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data then needs to be cleaned up and organized for visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace missing data with \"null\"\n",
    "def cleanNA(df):\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    df.dropna(axis=0, how='all', inplace=True)\n",
    "    df.fillna('null', inplace=True)\n",
    "\n",
    "#name columns appropriately\n",
    "def renameCols(name, df):\n",
    "    if name[-2:] == 'pm':\n",
    "        df.columns = ['id','session_num','time_clip','time', 'place', 'place_raw','scale']\n",
    "    else:\n",
    "        df.columns = ['id','session_num', 'su_num','time_clip_start', 'time_clip_end', 'time_start', 'time_end', 'place', 'place_raw', 'scale', 'journey']\n",
    "\n",
    "def cleanVals(name, df):\n",
    "    df['scale'] = df.scale.str.lower()\n",
    "    df.loc[df['scale'].str.contains('unknown', case=False), 'scale'] = 'null'\n",
    "    df['scale'].replace('city / area\\n', 'city / area', inplace=True)\n",
    "    #remove rows that are basically empty (make sure there are no actual story units containing no start and end-time, of course, but this should not be the case)\n",
    "    if name[-2:] == 'su':\n",
    "        nullList = df.loc[df['time_start'].str.contains('null', case=False) & df['time_end'].str.contains('null', case=False)].index\n",
    "        for null in nullList:\n",
    "            df.drop(null, inplace=True)\n",
    "    #add more cleaning functions if needed\n",
    "    \n",
    "def timeVals(name, df):\n",
    "    if name[-2:] == 'pm':\n",
    "        df['time'] = pd.to_datetime(df.time, format='%H:%M:%S')\n",
    "    else:\n",
    "        df['time_start'] = pd.to_datetime(df.time_start, format='%H:%M:%S')\n",
    "        df['time_end'] = pd.to_datetime(df.time_end, format='%H:%M:%S')\n",
    "\n",
    "def newCols(name, df):    \n",
    "    if name[-2:] == 'su':\n",
    "        df.loc[df['journey'].str.contains('journey', case=False, na=False), 'scale'] = 'journey' #give scale \"journey\" to units that are journeys\n",
    "    df['scale_order'] = df['scale']\n",
    "    df.loc[df['scale'].str.contains('journey'), 'scale_order'] = '1'\n",
    "    df.loc[df['scale'].str.contains('local'), 'scale_order'] = '3'\n",
    "    df.loc[df['scale'].str.contains('very local'), 'scale_order'] = '2'    \n",
    "    df.loc[df['scale'].str.contains('city / area'), 'scale_order'] = '4'\n",
    "    df.loc[df['scale'].str.contains('region'), 'scale_order'] = '5'\n",
    "    df.loc[df['scale'].str.contains('country'), 'scale_order'] = '6'\n",
    "    df.loc[df['scale'].str.contains('continent'), 'scale_order'] = '7'\n",
    "    df.loc[df['scale'].str.contains('null'), 'scale_order'] = '8'\n",
    "    df['scale_order'] = df.scale_order.astype(int)\n",
    "\n",
    "for k, v in d1.items():\n",
    "    cleanNA(v)\n",
    "    renameCols(k, v)\n",
    "    cleanVals(k, v)\n",
    "    timeVals(k, v)\n",
    "    newCols(k, v)\n",
    "\n",
    "for k, v in d2.items():\n",
    "    cleanNA(v)\n",
    "    renameCols(k, v)\n",
    "    cleanVals(k, v)\n",
    "    timeVals(k, v)\n",
    "    newCols(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate some statistics\n",
    "\n",
    "- create new column\n",
    "- calculate number of rows with 'time' val falling between time_start and time_end vals (>=,<). When a mention occurs at the temporal breakpoint between two story units, the story unit that follows the first is the one that the mention becomes associated with.\n",
    "- insert val into new column\n",
    "\n",
    "- create new column\n",
    "- calculate number of rows with 'time' val falling between time_start and time_end vals (><) that have the same 'place' val\n",
    "- insert val into new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcStats(name1, df1, name2, df2):\n",
    "    \n",
    "    r_pm = range(len(df1.index))\n",
    "    r_su = range(len(df2.index))\n",
    "    \n",
    "    df2['mention_freq'] = 0\n",
    "    df2['mention_index'] = ''\n",
    "    df2['mention_places'] = ''\n",
    "    \n",
    "    df2['mention_match_freq'] = 0\n",
    "    df2['mention_match_index'] = ''\n",
    "    df2['mention_match_places'] = ''\n",
    "    \n",
    "    df2['mention_coarser_match_freq'] = 0\n",
    "    df2['mention_coarser_match_index'] = ''\n",
    "    df2['mention_coarser_match_places'] = ''\n",
    "    \n",
    "    df2['mention_finer_match_freq'] = 0\n",
    "    df2['mention_finer_match_index'] = ''\n",
    "    df2['mention_finer_match_places'] = ''\n",
    "    \n",
    "    for i in r_su:\n",
    "        a = dates.date2num(df2.iloc[i]['time_start'])\n",
    "        b = dates.date2num(df2.iloc[i]['time_end'])\n",
    "        place1 = df2.iloc[i]['place']\n",
    "        x = 0\n",
    "        x1 = ''\n",
    "        x11 = ''\n",
    "        y = 0\n",
    "        y1 = ''\n",
    "        y11 = ''\n",
    "        y2 = 0\n",
    "        y21 = ''\n",
    "        y22 = ''\n",
    "        y3 = 0\n",
    "        y31 = ''\n",
    "        y32 = ''\n",
    "        \n",
    "        for j in r_pm:\n",
    "            c = dates.date2num(df1.iloc[j]['time'])\n",
    "            if (c >= a) and (c < b):\n",
    "                _id = str(df1.iloc[j]['id'])\n",
    "                place2 = df1.iloc[j]['place']\n",
    "                \n",
    "                x += 1\n",
    "                x1 += (\";\" + _id)\n",
    "                x11 += (\";\" + place2)\n",
    "                if place2 == place1:\n",
    "                    y += 1\n",
    "                    y1 += (\";\" + _id)\n",
    "                    y11 += (\";\" + place2)\n",
    "                elif (place1 in place2) and (place1 != place2):\n",
    "                    y2 += 1\n",
    "                    y21 += (\";\" + _id)\n",
    "                    y22 += (\";\" + place2)\n",
    "                elif (place2 in place1) and (place2 != place1):\n",
    "                    y3 += 1\n",
    "                    y31 += (\";\" + _id)\n",
    "                    y32 += (\";\" + place2)\n",
    "                else:\n",
    "                    pass\n",
    "        else:\n",
    "            df2.iloc[i, df2.columns.get_loc('mention_freq')] = x\n",
    "            df2.iloc[i, df2.columns.get_loc('mention_index')] = x1\n",
    "            df2.iloc[i, df2.columns.get_loc('mention_places')] = x11\n",
    "            \n",
    "            df2.iloc[i, df2.columns.get_loc('mention_match_freq')] = y\n",
    "            df2.iloc[i, df2.columns.get_loc('mention_match_index')] = y1\n",
    "            df2.iloc[i, df2.columns.get_loc('mention_match_places')] = y11\n",
    "            \n",
    "            df2.iloc[i, df2.columns.get_loc('mention_coarser_match_freq')] = y2\n",
    "            df2.iloc[i, df2.columns.get_loc('mention_coarser_match_index')] = y21\n",
    "            df2.iloc[i, df2.columns.get_loc('mention_coarser_match_places')] = y22\n",
    "            \n",
    "            df2.iloc[i, df2.columns.get_loc('mention_finer_match_freq')] = y3\n",
    "            df2.iloc[i, df2.columns.get_loc('mention_finer_match_index')] = y31\n",
    "            df2.iloc[i, df2.columns.get_loc('mention_finer_match_places')] = y32\n",
    "\n",
    "for (k1, v1), (k2, v2) in zip(sorted(d1.items()), sorted(d2.items())):\n",
    "    calcStats(k1, v1, k2, v2)\n",
    "    v1.to_csv(k1 + '.csv', sep=',', index=False, encoding='latin-1')\n",
    "    v2.to_csv(k2 + '.csv', sep=',', index=False, encoding='latin-1')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
